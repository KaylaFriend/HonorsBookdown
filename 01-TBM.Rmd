# Classification Trees

```{r echo=FALSE}
library(readr)
library(rpart)
library(rpart.plot)
library(tidyverse)
library(caret)

url <- "https://assets.datacamp.com/production/repositories/710/datasets/b649085c43111c83ba7ab6ec172d83cdc14a2942/credit.csv"
  if(!file.exists("./credit.csv")){download.file(url, destfile = "./credit.csv")}

credit <- read_csv("./credit.csv")
creditsub <- credit %>%
  select(months_loan_duration, percent_of_income, years_at_residence, age, default)
```

## Welcome to the Course{-}

<iframe src="https://drive.google.com/file/d/1NpANMsB2mgYs5Hg9T1mR7p2QnZZdV0DQ/preview" width="640" height="480"></iframe>

_______________

A classification tree is a decision tree that performs a classification (vs regression) task.## Build a Classification Tree

Let's get started and build our first classification tree. 

You will train a decision tree model to understand which loan applications are at higher risk of default using a subset of the [German Credit Dataset](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29). The response variable, `default`, indicates whether the loan went into a default or not, which means this is a binary classification problem (there are just two classes).

You will use the `rpart` package to fit the decision tree and the `rpart.plot` package to visualize the tree.

______________

### Exercise{-}

The data frame `creditsub` is in the workspace. This data frame is a subset of the original German Credit Dataset, which we will use to train our first classification tree model.

* Take a look at the data using the `str()` function.

```{r}
str(creditsub)
```

* In R, formulas are used to model the response as a function of some set of predictors, so the formula here is `default ~ .`, which means use all columns (except the response column) as predictors. Fit the classification decision tree using the `rpart()` function from the `rpart` package. In the `rpart()` function, note that you'll also have to provide the training data frame.

```{r}
credit_model <- rpart(formula = default ~ ., 
                      data = creditsub, 
                      method = "class")
```

* Using the model object that you create, plot the decision tree model using the `rpart.plot()` function from the `rpart.plot` package.

```{r}
rpart.plot(x = credit_model, yesno = 2, type = 0, extra = 0)
```



_____________

## Introduction to Classification Trees

<iframe src="https://drive.google.com/file/d/1Mz2Scq6UbFRBrASmXj3kOzO7zIr7nd8F/preview" width="640" height="480"></iframe>

______________

What are some advantages of using tree-based methods over other supervised learning methods?

* Model interpretability (easy to understand why a prediction is made).
* Model performance (trees have superior performance compared to other machine learning algorithms).
* No pre-processing (e.g. normalization) of the data is required.
* **1 and 3 are true.**

_____________

## Prediction with a Classification Tree

Let's use the decision tree that you trained in the first exercise. The tree predicts whether a loan applicant will default on their loan (or not).

Assume we have a loan applicant who:

is applying for a 20-month loan
is requesting a loan amount that is 2% of their income
is 25 years old
After following the correct path down the tree for this individual's set of data, you will end up in a "Yes" or "No" bucket (in tree terminology, we'd call this a "leaf") which represents the predicted class. Ending up in a "Yes" leaf means that the model predicts that this individual will default on their loan, where as a "No" prediction means that they will not default on their loan.

Starting with the top node of the tree, you must evaluate a query about a particular attribute of your data point (e.g. is `months_loan_duration < 44`?). If the answer is yes, then you go to the left at the split; if the answer is no, then you will go right. At the next node you repeat the process until you end up in a leaf node, at which point you'll have a predicted class for your data point.

```{r echo=FALSE}
rpart.plot(x = credit_model, yesno = 2, type = 0, extra = 0)
```

According to the model this person will default on their loan.

_____________

## Overview of the Modelling Process

<iframe src="https://drive.google.com/file/d/1ca-ESb3KqG7IOJiGx4K9-YEnqshcuSkr/preview" width="640" height="480"></iframe>

_____________

### Exercise{-}

For this exercise, you'll randomly split the [German Credit Dataset]() into two pieces: a training set (80%) called `credit_train` and a test set (20%) that we will call `credit_test`. We'll use these two sets throughout the chapter. The `credit` data frame is loaded into the workspace.

* Define `n`, the number of rows in the `credit` data frame.

```{r}
# Total number of rows in the credit data frame
n <- nrow(credit)
```

* Define `n_train` to be ~80% of `n`.

```{r}
# Number of rows for the training set (80% of the dataset)
n_train <- round(.8 * n)
```

* Set a seed (for reproducibility) and then sample `n_train` rows to define the set of training set indices.

```{r}
# Create a vector of indices which is an 80% random sample
set.seed(123)
train_indices <- sample(1:n, n_train)
```

* Using row indices, subset the credit data frame to create two new datasets: `credit_train` and `credit_test`

```{r}
# Subset the credit data frame to training indices only
credit_train <- credit[train_indices, ]  
  
# Exclude the training indices to create the test set
credit_test <- credit[-train_indices, ]
```

_______________

### Exercise: Train a Classification Tree{-}

In this exercise, you will train a model on the newly created training set and print the model object to get a sense of the results.

* Train a classification tree using the `credit_train` data frame.

```{r}
# Train the model (to predict 'default')
credit_model <- rpart(formula = default ~ ., 
                      data = credit_train, 
                      method = "class")
```

* Look at the model output by printing the model object.

```{r}
# Look at the model output                      
print(credit_model)
```

_______________

## Evaluating Classification Model Performance

<iframe src="https://drive.google.com/file/d/1TsfXThq_VqGzJ_Kwj76Jz1fNSmATjNOE/preview" width="640" height="480"></iframe>

_______________

## Compute confusion matrix

As discussed in the previous video, there are a number of different metrics by which you can measure the performance of a classification model. In this exercise, we will evaluate the performance of the model using test set classification error. A confusion matrix is a convenient way to examine the per-class error rates for all classes at once.

The confusionMatrix() function from the caret package prints both the confusion matrix and a number of other useful classification metrics such as "Accuracy" (fraction of correctly classified instances).

________________

### Exercise{-}

The caret package has been loaded for you.

* Generate class predictions for the `credit_test` data frame using the `credit_model` object.

```{r}
# Generate predicted classes using the model object
class_prediction <- predict(object = credit_model,  
                        newdata = credit_test,   
                        type = "class") 
```

* Using the `caret::confusionMatrix()` function, compute the confusion matrix for the test set.

```{r}
# Calculate the confusion matrix for the test set
caret::confusionMatrix(data = class_prediction,       
                reference = credit_test$default)
```

______________

## Use of Splitting Criterion in Trees

<iframe src="https://drive.google.com/file/d/1ui1eCpffCwFvV6HgWBibyOJT3KU3cZVg/preview" width="640" height="480"></iframe>

______________

## Compare models with a different splitting criterion{-}

Train two models that use a different splitting criterion and use the validation set to choose a "best" model from this group. To do this you'll use the `parms` argument of the `rpart()` function. This argument takes a named list that contains values of different parameters you can use to change how the model is trained. Set the parameter `split` to control the splitting criterion.

_______________

### Exercise{-}

The datasets `credit_test` and `credit_train` have already been loaded for you.

* Train a model, splitting the tree based on gini index.

```{r}
# Train a gini-based model
credit_model1 <- rpart(formula = default ~ ., 
                       data = credit_train, 
                       method = "class",
                       parms = list(split = "gini"))
```

* Train a model, splitting the tree based on information index.

```{r}
# Train an information-based model
credit_model2 <- rpart(formula = default ~ ., 
                       data = credit_train, 
                       method = "class",
                       parms = list(split = "information"))
```

* Generate predictions on the validation set using both models.

```{r}
# Generate predictions on the validation set using the gini model
pred1 <- predict(object = credit_model1,
                 newdata = credit_test,
                 type = "class")    

# Generate predictions on the validation set using the information model
pred2 <- predict(object = credit_model2, 
                 newdata = credit_test,
                 type = "class")
```

* Classification error is the fraction of incorrectly classified instances. Compute and compare the test set classification error of the two models by using the `ce()` function.

```{r}
# Compare classification error
ce(actual = credit_test$default, 
     predicted = pred1)
ce(actual = credit_test$default, 
     predicted = pred2) 
```

________________

