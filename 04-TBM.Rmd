# Random Forests

```{r echo=FALSE}
library(randomForest)
```

## Introduction to Random Forests

<iframe src="https://drive.google.com/file/d/1phMANteaur-rlsCADNKrEt-ncKLYQSWj/preview" width="640" height="480"></iframe>

____________

### Bagged trees vs. Random Forest

What is the main difference between bagged trees and the Random Forest algorithm?

* In Random Forest, the decision trees are trained on a random subset of the rows, but in bagging, they use all the rows.

* **In Random Forest, only a subset of features are selected at random at each split in a decision tree. In bagging, all features are used.**

* In Random Forest, there is randomness. In bagging, there is no randomness.

____________

## Train a Random Forest model

Here you will use the `randomForest()` function from the **randomForest** package to train a Random Forest classifier to predict loan default.

____________

### Exercise{-}

The `credit_train` and `credit_test` datasets (from Chapter 1 & 3) are already loaded in the workspace.

* Use the `randomForest::randomForest()` function to train a Random Forest model on the `credit_train` dataset.

```{r}
# Train a Random Forest
set.seed(1)  # for reproducibility
credit_model <- randomForest(formula = default ~ ., 
                             data = credit_train)
```

* The formula used to define the model is the same as in previous chapters -- we want to predict "default" as a function of all the other columns in the training set.

* Inspect the model output.

```{r}
# Print the model output                             
print(credit_model)
```

_______________

## Understanding Random Forest Model Output

<iframe src="https://drive.google.com/file/d/1nzJGmi-4ykBBuM4UyUvVvZYBnQ1F94Nv/preview" width="640" height="480"></iframe>

_______________

### Evaluate out-of-bag error

Here you will plot the OOB error as a function of the number of trees trained, and extract the final OOB error of the Random Forest model from the trained model object.

_______________

### Exercise{-}

The `credit_model` trained in the previous exercise is loaded in the workspace.

* Get the OOB error rate for the Random Forest model.

```{r}
# Grab OOB error matrix & take a look
err <- credit_model$err.rate
head(err)

# Look at final OOB error rate (last row in err matrix)
oob_err <- err[500, "OOB"]
print(oob_err)
```

* Plot the OOB error rate against the number of trees in the forest.

```{r}
# Plot the model trained in the previous exercise
plot(credit_model)

# Add a legend since it doesn't have one by default
legend(x = "right", 
       legend = colnames(err),
       fill = 1:ncol(err))
```

_______________

### Evaluate model performance on a test set

Use the `caret::confusionMatrix()` function to compute test set accuracy and generate a confusion matrix. Compare the test set accuracy to the OOB accuracy.

_______________

### Exercise{-}

* Generate class predictions for the `credit_test` data frame using the `credit_model` object.

```{r}
# Generate predicted classes using the model object
class_prediction <- predict(object = credit_model,   # model object 
                            newdata = credit_test,  # test dataset
                            type = "class") # return classification labels
```

* Using the `caret::confusionMatrix()` function, compute the confusion matrix for the test set.

```{r}
# Calculate the confusion matrix for the test set
cm <- confusionMatrix(data = class_prediction,       # predicted classes
                      reference = credit_test$default)  # actual classes
print(cm)
```

* Compare the test set accuracy reported from the confusion matrix to the OOB accuracy. The OOB error is stored in `oob_err`, which is already in your workspace, and so OOB accuracy is just `1 - oob_err`.

```{r}
# Compare test set accuracy to OOB accuracy
paste0("Test Accuracy: ", cm$overall[1])
paste0("OOB Accuracy: ", 1 - oob_err)
```

_______________







