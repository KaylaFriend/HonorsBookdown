# Random Forests

```{r include=FALSE}
library(randomForest)
library(dplyr)
library(readr)
library(rpart)
library(rpart.plot)
library(tidyverse)
library(caret)
library(Metrics)

url <- "https://assets.datacamp.com/production/repositories/710/datasets/b649085c43111c83ba7ab6ec172d83cdc14a2942/credit.csv"
  if(!file.exists("./credit.csv")){download.file(url, destfile = "./credit.csv")}

credit <- read.csv("./credit.csv")

creditsub <- credit %>%
  select(months_loan_duration, percent_of_income, years_at_residence, age, default)
n <- nrow(creditsub)
n_train <- round(.8 * n)
set.seed(123)
train_indices <- sample(1:n, n_train)
# Subset the credit data frame to training indices only
credit_train <- credit[train_indices, ]  
  
# Exclude the training indices to create the test set
credit_test <- credit[-train_indices, ]

credit_train$default <- as.factor(credit_train$default)

library(dplyr)
credit_Train <- credit_train %>% mutate_if(is.character, as.factor)
credit_Test <- credit_test %>% mutate_if(is.character, as.factor)
```

## Introduction to Random Forests

<iframe src="https://drive.google.com/file/d/1phMANteaur-rlsCADNKrEt-ncKLYQSWj/preview" width="640" height="480"></iframe>

____________

### Bagged trees vs. Random Forest

What is the main difference between bagged trees and the Random Forest algorithm?

* In Random Forest, the decision trees are trained on a random subset of the rows, but in bagging, they use all the rows.

* **In Random Forest, only a subset of features are selected at random at each split in a decision tree. In bagging, all features are used.**

* In Random Forest, there is randomness. In bagging, there is no randomness.

____________

## Train a Random Forest model

Here you will use the `randomForest()` function from the **randomForest** package to train a Random Forest classifier to predict loan default.

____________

### Exercise{-}

The `credit_train` and `credit_test` datasets (from Chapter 1 & 3) are already loaded in the workspace.

* Use the `randomForest::randomForest()` function to train a Random Forest model on the `credit_train` dataset.

```{r comment=NA}
# Train a Random Forest
set.seed(1)  # for reproducibility
credit_model <- randomForest(default ~ ., 
                             credit_Train)
```

* The formula used to define the model is the same as in previous chapters -- we want to predict "default" as a function of all the other columns in the training set.

* Inspect the model output.

```{r}
# Print the model output                             
print(credit_model)
```

_______________

## Understanding Random Forest Model Output

<iframe src="https://drive.google.com/file/d/1nzJGmi-4ykBBuM4UyUvVvZYBnQ1F94Nv/preview" width="640" height="480"></iframe>

_______________

### Evaluate out-of-bag error

Here you will plot the OOB error as a function of the number of trees trained, and extract the final OOB error of the Random Forest model from the trained model object.

_______________

### Exercise{-}

The `credit_model` trained in the previous exercise is loaded in the workspace.

* Get the OOB error rate for the Random Forest model.

```{r comment=NA}
# Grab OOB error matrix & take a look
err <- credit_model$err.rate
head(err)

# Look at final OOB error rate (last row in err matrix)
oob_err <- err[500, "OOB"]
print(oob_err)
```

* Plot the OOB error rate against the number of trees in the forest.

```{r}
# Plot the model trained in the previous exercise
plot(credit_model)

# Add a legend since it doesn't have one by default
legend(x = "right", 
       legend = colnames(err),
       fill = 1:ncol(err))
```

_______________

### Evaluate model performance on a test set

Use the `caret::confusionMatrix()` function to compute test set accuracy and generate a confusion matrix. Compare the test set accuracy to the OOB accuracy.

_______________

### Exercise{-}

* Generate class predictions for the `credit_test` data frame using the `credit_model` object.

```{r}
# Generate predicted classes using the model object
class_prediction <- predict(object = credit_model,   # model object 
                            newdata = credit_Test,  # test dataset
                            type = "class") # return classification labels
```

* Using the `caret::confusionMatrix()` function, compute the confusion matrix for the test set.

```{r comment=NA}
# Calculate the confusion matrix for the test set
cm <- confusionMatrix(data = class_prediction,       # predicted classes
                      reference = credit_Test$default)  # actual classes
print(cm)
```

* Compare the test set accuracy reported from the confusion matrix to the OOB accuracy. The OOB error is stored in `oob_err`, which is already in your workspace, and so OOB accuracy is just `1 - oob_err`.

```{r comment=NA}
# Compare test set accuracy to OOB accuracy
paste0("Test Accuracy: ", cm$overall[1])
paste0("OOB Accuracy: ", 1 - oob_err)
```

_______________

## OOB Error vs. Test Set Error

<iframe src="https://drive.google.com/file/d/1g08rV8Tzr5VhimAPIv6B0lnWuIGsfNE5/preview" width="640" height="480"></iframe>

_______________

### Advantage of OOB error

What is the main advantage of using OOB error instead of validation or test error?

* Tuning the model hyperparameters using OOB error will lead to a better model.

* **If you evaluate your model using OOB error, then you don't need to create a separate test set.**

* OOB error is more accurate than test set error.

_______________

### Evaluate Test Set AUC

In Chapter 3, we learned about the [AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve) metric for evaluating binary classification models. In this exercise, you will compute test set AUC for the Random Forest model.

_______________

### Exercise{-}

* Use the `predict()` function with `type = "prob"` to generate numeric predictions on the `credit_test` dataset.

```{r comment=NA}
# Generate predictions on the test set
pred <- predict(object = credit_model, 
                newdata = credit_Test,
                type = "prob")

# `pred` is a matrix
class(pred)
                
# Look at the pred format
head(pred) 
```
```{r}
credit_Model <- randomForest(default ~ ., 
                             credit_Train)
rf_preds <- predict(object = credit_Model, 
                newdata = credit_Test)
```


* Compute the AUC using the `auc()` function from the **Metrics** package.

```{r comment=NA}
# Compute the AUC (`actual` must be a binary 1/0 numeric vector)
auc(actual = ifelse(credit_Test$default == "yes", 1, 0), 
    predicted = pred[,"yes"])
```

_____________

## Tuning a Random Forest Model

<iframe src="https://drive.google.com/file/d/1fRnfgVML4fdzTDHEgydmxaEBp5A9noq7/preview" width="640" height="480"></iframe>

_____________

### Tuning a Random Forest via mtry

In this exercise, you will use the `randomForest::tuneRF()` to tune `mtry` (by training several models). This function is a specific utility to tune the `mtry` parameter based on OOB error, which is helpful when you want a quick & easy way to tune your model. A more generic way of tuning Random Forest parameters will be presented in the following exercise.

____________

### Exercise{-}

* Use the `tuneRF()` function in place of the `randomForest()` function to train a series of models with different `mtry` values and examine the the results.
  * Note that (unfortunately) the `tuneRF()` interface does not support the           typical formula input that we've been using, but instead uses two arguments,      `x` (matrix or data frame of predictor variables) and `y` (response vector;       must be a factor for classification).
* The `tuneRF()` function has an argument, `ntreeTry` that defaults to 50 trees. Set `nTreeTry = 500` to train a random forest model of the same size as you previously did.

```{r comment=NA}
# Execute the tuning process
set.seed(1)              
res <- tuneRF(x = subset(credit_Train, select = -default),
              y = credit_Train$default,
              ntreeTry = 500)
```

* After tuning the forest, this function will also plot model performance (OOB error) as a function of the `mtry` values that were evaluated.
  * Keep in mind that if we want to evaluate the model based on AUC instead of        error (accuracy), then this is not the best way to tune a model, as the           selection only considers (OOB) error.
  
```{r comment=NA}
# Look at results
print(res)

# Find the mtry value that minimizes OOB Error
mtry_opt <- res[,"mtry"][which.min(res[,"OOBError"])]
print(mtry_opt)

```

_____________

### Tuning a Random Forest via tree depth

In Chapter 2, we created a manual grid of hyperparameters using the `expand.grid()` function and wrote code that trained and evaluated the models of the grid in a loop. In this exercise, you will create a grid of `mtry`, `nodesize` and `sampsize` values. In this example, we will identify the "best model" based on OOB error. The best model is defined as the model from our grid which minimizes OOB error.

Keep in mind that there are other ways to select a best model from a grid, such as choosing the best model based on validation AUC. However, for this exercise, we will use the built-in OOB error calculations instead of using a separate validation set.

______________

### Exercise{-}

* Create a grid of `mtry`, `nodesize` and `sampsize` values.

```{r}
# Establish a list of possible values for mtry, nodesize and sampsize
mtry <- seq(4, ncol(credit_Train) * 0.8, 2)
nodesize <- seq(3, 8, 2)
sampsize <- nrow(credit_Train) * c(0.7, 0.8)

# Create a data frame containing all combinations 
hyper_grid <- expand.grid(mtry = mtry, nodesize = nodesize, sampsize = sampsize)

# Create an empty vector to store OOB error values
oob_err <- c()
```

* Write a simple loop to train all the models and choose the best one based on OOB error.

```{r}
# Write a loop over the rows of hyper_grid to train the grid of models
for (i in 1:nrow(hyper_grid)) {

    # Train a Random Forest model
    model <- randomForest(formula = default ~ ., 
                          data = credit_Train,
                          mtry = hyper_grid$mtry[i],
                          nodesize = hyper_grid$nodesize[i],
                          sampsize = hyper_grid$sampsize[i])
                          
    # Store OOB error for the model                      
    oob_err[i] <- model$err.rate[nrow(model$err.rate), "OOB"]
}
```

* Print the set of hyperparameters which produced the best model.

```{r comment=NA}
# Identify optimal set of hyperparmeters based on OOB error
opt_i <- which.min(oob_err)
print(hyper_grid[opt_i,])
```

_____________

