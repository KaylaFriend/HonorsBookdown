# Bagged Trees

```{r include=FALSE}
library(readr)
library(rpart)
library(rpart.plot)
library(tidyverse)
library(caret)
library(ipred)
library(Metrics)

url <- "https://assets.datacamp.com/production/repositories/710/datasets/b649085c43111c83ba7ab6ec172d83cdc14a2942/credit.csv"
  if(!file.exists("./credit.csv")){download.file(url, destfile = "./credit.csv")}

credit <- read_csv("./credit.csv")
creditsub <- credit %>%
  select(months_loan_duration, percent_of_income, years_at_residence, age, default)

n <- nrow(creditsub)
n_train <- round(.8 * n)
set.seed(123)
train_indices <- sample(1:n, n_train)
# Subset the credit data frame to training indices only
credit_train <- credit[train_indices, ]  
  
# Exclude the training indices to create the test set
credit_test <- credit[-train_indices, ]
```

## Introduction to Bagged Trees

<iframe src="https://drive.google.com/file/d/1DhOLqbdyl-XQxWy7uebvQ9QDo5F3skXO/preview" width="640" height="480"></iframe>

_____________

### Advantages of bagged trees

What are the advantages of bagged trees compared to a single tree?

* Increases the accuracy of the resulting predictions

* Easier to interpret the resulting model

* Reduces variance by averaging a set of observations

* 1 and 2 are correct

* **1 and 3 are correct**

* 2 and 3 are correct

_____________

## Train a Bagged Tree Model

Let's start by training a bagged tree model. You'll be using the `bagging()` function from the `ipred` package. The number of bagged trees can be specified using the `nbagg` parameter, but here we will use the default (25).

If we want to estimate the model's accuracy using the "out-of-bag" (OOB) samples, we can set the the `coob` parameter to `TRUE`. The OOB samples are the training obsevations that were not selected into the bootstrapped sample (used in training). Since these observations were not used in training, we can use them instead to evaluate the accuracy of the model (done automatically inside the `bagging()` function).

_____________

### Exercise{-}

The `credit_train` and `credit_test` datasets from Chapter 1 are already loaded in the workspace.

* Use the `bagging()` function to train a bagged tree model.

```{r}
# Bagging is a randomized model, so let's set a seed (123) for reproducibility
set.seed(123)

# Train a bagged model
credit_model <- bagging(formula = factor(default) ~ ., 
                        data = credit_train,
                        coob = TRUE)
```

* Inspect the model by printing it.

```{r comment=NA}
# Print the model
print(credit_model)
```

______________

## Evaluating the Bagged Tree Performance

<iframe src="https://drive.google.com/file/d/1fo_Ons7vxGcmJYvQIAizhbWuNRiiV-Kj/preview" width="640" height="480"></iframe>

_____________

### Prediction and confusion matrix

As you saw in the video, a confusion matrix is a very useful tool for examining all possible outcomes of your predictions (true positive, true negative, false positive, false negative).

In this exercise, you will predict those who will default using bagged trees. You will also create the confusion matrix using the `confusionMatrix()` function from the **caret** package.

It's always good to take a look at the output using the `print()` function.

_____________

### Exercise{-}

The fitted model object, `credit_model`, is already in your workspace.

* Use the `predict()` function with `type = "class"` to generate predicted labels on the `credit_test` dataset.

```{r}
# Generate predicted classes using the model object
class_prediction <- predict(object = credit_model, 
                            newdata = credit_test,  
                            type = "class") 
```

* Take a look at the prediction using the `print()` function.

```{r comment=NA}
# Print the predicted classes
print(class_prediction)
```

* Calculate the confusion matrix using the `confusionMatrix` function.

```{r comment=NA}
# Calculate the confusion matrix for the test set
confusionMatrix(data = class_prediction,         
                reference = factor(credit_test$default))  
```

_____________

### Predict on a Test Set and Compute AUC

In binary classification problems, we can predict numeric values instead of class labels. In fact, class labels are created only after you use the model to predict a raw, numeric, _predicted value_ for a test point.

The _predicted label_ is generated by applying a threshold to the _predicted value_, such that all tests points with predicted value greater than that threshold get a predicted label of "1" and, points below that threshold get a predicted label of "0".

In this exercise, generate predicted values (rather than class labels) on the test set and evaluate performance based on [AUC (Area Under the ROC Curve)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve). The AUC is a common metric for evaluating the discriminatory ability of a binary classification model.

_______________

### Exercise{-}

* Use the `predict()` function with `type = "prob"` to generate numeric predictions on the `credit_test` dataset.

```{r comment=NA}
# Generate predictions on the test set
pred <- predict(object = credit_model,
                newdata = credit_test,
                type = "prob")

# `pred` is a matrix
class(pred)
                
# Look at the pred format
head(pred)
```

* Compute the AUC using the `auc()` function from the **Metrics** package.

```{r comment=NA}
# Compute the AUC (`actual` must be a binary (or 1/0 numeric) vector)
auc(actual = ifelse(credit_test$default == "yes", 1, 0), 
    predicted = pred[,"yes"])  
```

________________

## Using `caret` for Cross-Validating Models

<iframe src="https://drive.google.com/file/d/1nebe1SkHqeawp-41d9vCy5Pf0BPHGaxE/preview" width="640" height="480"></iframe>

________________

### Cross-validate a bagged tree model in caret

Use `caret::train()` with the `"treebag"` method to train a model and evaluate the model using cross-validated AUC. The **caret** package allows the user to easily cross-validate any model across any relevant performance metric. In this case, we will use 5-fold cross validation and evaluate cross-validated AUC (Area Under the ROC Curve).

_______________

### Exercise{-}

The `credit_train` dataset is in your workspace. You will use this data frame as the training data.

* First specify a `ctrl` object, which is created using the `caret::trainControl()` function.

```{r}
# Specify the training configuration
ctrl <- trainControl(method = "cv",     # Cross-validation
                     number = 5,      # 5 folds
                     classProbs = TRUE,                  # For AUC
                     summaryFunction = twoClassSummary)  # For AUC
```

* In the `trainControl()` function, you can specify many things. We will set: `method = "cv"`, `number = 5` for 5-fold cross-validation. Also, two options that are required if you want to use AUC as the metric: `classProbs = TRUE` and `summaryFunction = twoClassSummary`.

```{r comment=NA}
# Cross validate the credit model using "treebag" method; 
# Track AUC (Area under the ROC curve)
set.seed(1)  # for reproducibility
credit_caret_model <- train(default ~ .,
                            data = credit_train, 
                            method = "treebag",
                            metric = "ROC",
                            trControl = ctrl)

# Look at the model object
print(credit_caret_model)

# Inspect the contents of the model list 
names(credit_caret_model)

# Print the CV AUC
credit_caret_model$results[,"ROC"]
```

________________

### Generate predictions from the caret model

Generate predictions on a test set for the caret model.

* First generate predictions on the `credit_test` data frame using the `credit_caret_model` object.

```{r}
# Generate predictions on the test set
predc <- predict(object = credit_caret_model, 
                newdata = credit_test,
                type = "prob")
```
```{r include=FALSE}
bag_preds <- predict(object = credit_caret_model, 
                newdata = credit_test)
```


* After generating test set predictions, use the `auc()` function from the `Metrics` package to compute AUC.

```{r comment=NA}
# Compute the AUC (`actual` must be a binary (or 1/0 numeric) vector)
auc(actual = ifelse(credit_test$default == "yes", 1, 0), 
                    predicted = predc[,"yes"])
```

________________

## Compare test set performance to CV performance

In this excercise, you will print test set AUC estimates that you computed in previous exercises. These two methods use the same code underneath, so the estimates should be very similar.

* The `credit_ipred_model_test_auc` object stores the test set AUC from the model trained using the `ipred::bagging()` function.

* The `credit_caret_model_test_auc` object stores the test set AUC from the model trained using the `caret::train()` function with `method = "treebag"`.

Lastly, we will print the 5-fold cross-validated estimate of AUC that is stored within the `credit_caret_model` object. This number will be a more accurate estimate of the true model performance since we have averaged the performance over five models instead of just one.

On small datasets like this one, the difference between test set model performance estimates and cross-validated model performance estimates will tend to be more pronounced. When using small data, it's recommended to use cross-validated estimates of performance because they are more stable.

________________

### Exercise{-}

* Print the object credit_ipred_model_test_auc.

```{r include=FALSE}
credit_ipred_model_test_auc <- auc(actual = ifelse(credit_test$default == "yes", 1, 0), 
    predicted = pred[,"yes"])  
```

```{r comment=NA}
# Print ipred::bagging test set AUC estimate
print(credit_ipred_model_test_auc)
```

* Print the object credit_caret_model_test_auc.

```{r include=FALSE}
credit_caret_model_test_auc <- auc(actual = ifelse(credit_test$default == "yes", 1, 0), 
                    predicted = predc[,"yes"])
```

```{r comment=NA}
# Print caret "treebag" test set AUC estimate
print(credit_caret_model_test_auc)
```

* Compare these to the 5-fold cross validated AUC.

```{r comment=NA}
# Compare to caret 5-fold cross-validated AUC
credit_caret_model$results[, "ROC"]
```

__________________
